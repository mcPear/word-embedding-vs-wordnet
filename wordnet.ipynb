{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plwn\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path\n",
    "from statistics import mean\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = plwn.load_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('wordnet/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('wordnet/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def build_synsets_tree():\n",
    "    s_edges = np.load('wordnet/edges_synsets.npy')\n",
    "    g = nx.DiGraph()\n",
    "    g.add_edges_from(s_edges)\n",
    "    \n",
    "    roots = [ n for n in g.nodes if len(list(g.predecessors(n))) == 0 ]\n",
    "    connected_roots = [('root', n) for n in roots]\n",
    "    g.add_edges_from(connected_roots)\n",
    "    \n",
    "    wn_synsets = wn.synsets()\n",
    "    edges_flatten = set([node for edge in s_edges for node in edge])\n",
    "    s_ids = set([s.id for s in wn_synsets])\n",
    "    free_nodes = s_ids.difference(edges_flatten)\n",
    "    free_edges = [('root', n) for n in free_nodes]\n",
    "    g.add_edges_from(free_edges)\n",
    "    \n",
    "    nx.write_gpickle(g, \"wordnet/wn_tree.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2s_map = load_obj('lemma_synset_mapping')\n",
    "g = nx.read_gpickle(\"wordnet/wn_tree.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_depths():\n",
    "    node_depths = [len(shortest_path(g, 'root', node)) for node in g.nodes]\n",
    "    max_depth = max(node_depths) # 28\n",
    "    avg_depth = round(mean(node_depths), 2) # 5.88\n",
    "    \n",
    "MAX_DEPTH = 28\n",
    "AVG_DEPTH = 5.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distance(first, second):\n",
    "    if check_if_common_synset(first, second):\n",
    "        return 1\n",
    "    \n",
    "    def init_branches(word):\n",
    "        return [[node, parent] for node in l2s_map[word] for parent in g.predecessors(node)]            \n",
    "    \n",
    "    fst_branches = init_branches(first)\n",
    "    snd_branches = init_branches(second)\n",
    "    \n",
    "    def expand_branches(branches):\n",
    "        new_branches = []\n",
    "        for branch in branches:\n",
    "            parents = list(g.predecessors(branch[-1]))\n",
    "            if len(parents) == 0:\n",
    "                new_branches.append(branch)\n",
    "            else:\n",
    "                for parent in parents:\n",
    "                    new_branch = branch + [parent]\n",
    "                    new_branches.append(new_branch)\n",
    "        return new_branches\n",
    "    \n",
    "    def common_nodes(fst_path, snd_path):\n",
    "        return len(set(fst_path).intersection(set(snd_path))) > 0\n",
    "    \n",
    "    def get_distance(fst_path, snd_path):\n",
    "        common_node = list(set(fst_path).intersection(set(snd_path)))[0]\n",
    "        return fst_path.index(common_node) + snd_path.index(common_node) + 1\n",
    "    \n",
    "    min_distance = None\n",
    "\n",
    "    while not min_distance:\n",
    "        fst_branches = expand_branches(fst_branches)\n",
    "        snd_branches = expand_branches(snd_branches)\n",
    "        \n",
    "        distances = [get_distance(fst, snd) for fst in fst_branches for snd in snd_branches if common_nodes(fst, snd)]\n",
    "        if len(distances) > 0:\n",
    "            min_distance = min(distances)\n",
    "    \n",
    "    return min_distance\n",
    "        \n",
    "def check_if_common_synset(first, second):\n",
    "    return len(set(l2s_map[first]).intersection(set(l2s_map[second]))) > 0\n",
    "\n",
    "def words_exists(first, second):\n",
    "    if first not in l2s_map:\n",
    "        print('Word ', first, 'not exist!')\n",
    "        return False\n",
    "    if second not in l2s_map:\n",
    "        print('Word ', second, 'not exist!')\n",
    "        return False        \n",
    "    return True\n",
    "\n",
    "def wordnet_similarity_measure(first, second):\n",
    "    if not words_exists(first, second):\n",
    "        return 0\n",
    "    distance = count_distance(first, second)\n",
    "    measure = -math.log(1.0 * distance / (2 * MAX_DEPTH))\n",
    "    return round(measure, 2)\n",
    "\n",
    "def wordnet_similarity_measure2(first, second):\n",
    "    if not words_exists(first, second):\n",
    "        return 0\n",
    "    distance = count_distance(first, second)\n",
    "    measure = -math.log(1.0 * distance / (2 * AVG_DEPTH))\n",
    "    if measure < 0:\n",
    "        measure = 0\n",
    "    return round(measure, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95\n",
      "0.39\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_similarity_measure(\"lodówka\", \"lodowisko\"))\n",
    "print(wordnet_similarity_measure2(\"lodówka\", \"lodowisko\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, k):\n",
    "    synsets = [wn.synset_by_id(int(s_id)) for s_id in l2s_map[word]]\n",
    "    synset_units = []\n",
    "    for syn in synsets:\n",
    "        synset_units.extend([lex_unit.lemma for lex_unit in syn.lexical_units])\n",
    "    synset_units = list(set(synset_units) - set([word]))\n",
    "    \n",
    "    synset_result = synset_units[:k]\n",
    "    if len(synset_result) < k:\n",
    "        parents = []\n",
    "        for s_id in l2s_map[word]:\n",
    "            parents.extend(g.predecessors(s_id))\n",
    "            \n",
    "        neighbors = [lex_unit.lemma for s_id in parents for lex_unit in wn.synset_by_id(int(s_id)).lexical_units]\n",
    "        neighbors_size = k - len(synset_result)\n",
    "        synset_result.extend(neighbors[:neighbors_size])\n",
    "    return synset_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chłodziarka', 'lodownia', 'urządzenie kuchenne']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('lodówka', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    test_data = np.load('test_data.npy')\n",
    "    results = []\n",
    "    for d_id, data in enumerate(test_data):\n",
    "        word_1 = data[0]\n",
    "        word_2 = data[1]\n",
    "        result_1 = wordnet_similarity_measure(word_1, word_2)\n",
    "        result_2 = wordnet_similarity_measure2(word_1, word_2)\n",
    "        \n",
    "#         print(d_id, datetime.now(), word_1, word_2, result_1, result_2)\n",
    "        results.append([word_1, word_2, result_1, result_2])\n",
    "#         np.save('backup_test_wordnet', results)\n",
    "    np.save('test_wordnet', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  obładowany not exist!\n",
      "Word  obładowany not exist!\n",
      "Word  niedawny not exist!\n",
      "Word  niedawny not exist!\n",
      "Word  ubrania not exist!\n",
      "Word  ubrania not exist!\n",
      "Word  nasiona not exist!\n",
      "Word  nasiona not exist!\n",
      "Word  mężczyźni not exist!\n",
      "Word  mężczyźni not exist!\n",
      "Word  meble not exist!\n",
      "Word  meble not exist!\n",
      "Word  spowiedż not exist!\n",
      "Word  spowiedż not exist!\n",
      "Word  wyobrażać not exist!\n",
      "Word  wyobrażać not exist!\n",
      "Word  pojawiać not exist!\n",
      "Word  pojawiać not exist!\n",
      "Word  modlić not exist!\n",
      "Word  modlić not exist!\n"
     ]
    }
   ],
   "source": [
    "run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corr():\n",
    "    test_data = np.load('test_data.npy')\n",
    "    df_test = pd.DataFrame(test_data, columns = ['word_1', 'word_2', 'simi_base'])\n",
    "    df_test['simi_base'] = df_test['simi_base'].astype(float)\n",
    "    \n",
    "    my_results = np.load('test_wordnet.npy')\n",
    "    df_my_results = pd.DataFrame(my_results, columns = ['word_1', 'word_2', 'simi_1', 'simi_2'])\n",
    "    df_my_results['simi_1'] = df_my_results['simi_1'].astype(float)\n",
    "    df_my_results['simi_2'] = df_my_results['simi_2'].astype(float)\n",
    "    \n",
    "    merge = pd.concat([df_test, df_my_results], axis=1, join='inner')[['simi_base', 'simi_1', 'simi_2']]\n",
    "    return merge.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simi_base</th>\n",
       "      <th>simi_1</th>\n",
       "      <th>simi_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>simi_base</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.449342</td>\n",
       "      <td>0.48493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>simi_1</td>\n",
       "      <td>0.449342</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.96381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>simi_2</td>\n",
       "      <td>0.484930</td>\n",
       "      <td>0.963810</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           simi_base    simi_1   simi_2\n",
       "simi_base   1.000000  0.449342  0.48493\n",
       "simi_1      0.449342  1.000000  0.96381\n",
       "simi_2      0.484930  0.963810  1.00000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plwn\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path\n",
    "from networkx.algorithms.simple_paths import all_simple_paths\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = plwn.load_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('wordnet/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('wordnet/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def build_synsets_tree():\n",
    "    s_edges = np.load('wordnet/edges_synsets.npy')\n",
    "    g = nx.DiGraph()\n",
    "    g.add_edges_from(s_edges)\n",
    "    roots = [ n for n in g.nodes if len(list(g.predecessors(n))) == 0 ]\n",
    "    connected_roots = [('root', n) for n in roots]\n",
    "    g.add_edges_from(connected_roots)\n",
    "    nx.write_gpickle(g, \"wordnet/wn_tree.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2s_map = load_obj('lemma_synset_mapping')\n",
    "g = nx.read_gpickle(\"wordnet/wn_tree.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_depths():\n",
    "    node_depths = [len(shortest_path(g, 'root', node)) for node in g.nodes]\n",
    "    max_depth = max(node_depths) # 28\n",
    "    avg_depth = round(mean(node_depths), 2) # 5.88\n",
    "    \n",
    "MAX_DEPTH = 28\n",
    "AVG_DEPTH = 5.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_distance(x, y):\n",
    "    x = set(x)\n",
    "    y = set(y)\n",
    "    distance = len(x.difference(y)) + len(y.difference(x))\n",
    "    return distance + 1\n",
    "    \n",
    "def count_distance(first, second):\n",
    "    if check_if_common_synset(first, second):\n",
    "        return 1\n",
    "    \n",
    "    fst_paths = []\n",
    "    snd_paths = []\n",
    "    for s_id in l2s_map[first]:\n",
    "        fst_paths.extend(list(all_simple_paths(g, 'root', s_id)))\n",
    "    for s_id in l2s_map[second]:\n",
    "        snd_paths.extend(list(all_simple_paths(g, 'root', s_id)))\n",
    "\n",
    "    distances = [paths_distance(x, y) for x in fst_paths for y in snd_paths]\n",
    "    return min(distances)\n",
    "\n",
    "def check_if_common_synset(first, second):\n",
    "    return len(set(l2s_map[first]).intersection(set(l2s_map[second]))) > 0\n",
    "\n",
    "def wordnet_similarity_measure(first, second):\n",
    "    distance = count_distance(first, second)\n",
    "    measure = -math.log(1.0 * distance / (2 * MAX_DEPTH))\n",
    "    return measure\n",
    "\n",
    "def wordnet_similarity_measure2(first, second):\n",
    "    distance = count_distance(first, second)\n",
    "    measure = -math.log(1.0 * distance / (2 * AVG_DEPTH))\n",
    "    if measure < 0:\n",
    "        measure = 0\n",
    "    return measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9459101490553135\n",
      "0.38526240079064494\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_similarity_measure(\"lodówka\", \"lodowisko\"))\n",
    "print(wordnet_similarity_measure2(\"lodówka\", \"lodowisko\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, k):\n",
    "    synsets = [wn.synset_by_id(int(s_id)) for s_id in l2s_map[word]]\n",
    "    synset_units = []\n",
    "    for syn in synsets:\n",
    "        synset_units.extend([lex_unit.lemma for lex_unit in syn.lexical_units])\n",
    "    synset_units = list(set(synset_units) - set([word]))\n",
    "    \n",
    "    synset_result = synset_units[:k]\n",
    "    if len(synset_result) < k:\n",
    "        parents = []\n",
    "        for s_id in l2s_map[word]:\n",
    "            parents.extend(g.predecessors(s_id))\n",
    "            \n",
    "        neighbors = [lex_unit.lemma for s_id in parents for lex_unit in wn.synset_by_id(int(s_id)).lexical_units]\n",
    "        neighbors_size = k - len(synset_result)\n",
    "        synset_result.extend(neighbors[:neighbors_size])\n",
    "    return synset_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lodownia', 'chłodziarka', 'urządzenie kuchenne']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('lodówka', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
